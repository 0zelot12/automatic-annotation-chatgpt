{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "from pet_document import PetDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './assets/pet_dataset.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./assets/pet_dataset.parquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/chatpgpt/lib/python3.11/site-packages/pandas/io/parquet.py:670\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[0m\n\u001b[1;32m    667\u001b[0m     use_nullable_dtypes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    668\u001b[0m check_dtype_backend(dtype_backend)\n\u001b[0;32m--> 670\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/chatpgpt/lib/python3.11/site-packages/pandas/io/parquet.py:394\u001b[0m, in \u001b[0;36mFastParquetImpl.read\u001b[0;34m(self, path, columns, filters, storage_options, filesystem, **kwargs)\u001b[0m\n\u001b[1;32m    389\u001b[0m     parquet_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m fsspec\u001b[38;5;241m.\u001b[39mopen(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(storage_options \u001b[38;5;129;01mor\u001b[39;00m {}))\u001b[38;5;241m.\u001b[39mfs\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(path):\n\u001b[1;32m    391\u001b[0m     \u001b[38;5;66;03m# use get_handle only when we are very certain that it is not a directory\u001b[39;00m\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;66;03m# fsspec resources can also point to directories\u001b[39;00m\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;66;03m# this branch is used for example when reading from non-fsspec URLs\u001b[39;00m\n\u001b[0;32m--> 394\u001b[0m     handles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    397\u001b[0m     path \u001b[38;5;241m=\u001b[39m handles\u001b[38;5;241m.\u001b[39mhandle\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/chatpgpt/lib/python3.11/site-packages/pandas/io/common.py:872\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    864\u001b[0m             handle,\n\u001b[1;32m    865\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    868\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    869\u001b[0m         )\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m--> 872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    873\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[1;32m    875\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './assets/pet_dataset.parquet'"
     ]
    }
   ],
   "source": [
    "df = pd.read_parquet(\"../assets/pet_dataset.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_simple_bar_chart(values, categories, xLabel, yLabel, title):\n",
    "    # Set the width of the bars\n",
    "    bar_width = 0.35\n",
    "\n",
    "    # Set the positions of the bars on the x-axis\n",
    "    bar_positions = np.arange(len(categories))\n",
    "\n",
    "    # Calculate the dynamic figure width based on the number of categories\n",
    "    figure_width = max(10, len(categories) * 0.8)\n",
    "\n",
    "    # Set the figure size dynamically\n",
    "    plt.figure(figsize=(figure_width, 6))  # Adjust the height as needed\n",
    "\n",
    "    # Create the bar plot\n",
    "    plt.bar(bar_positions, values, width=bar_width)\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel(xLabel)\n",
    "    plt.ylabel(yLabel)\n",
    "    plt.title(title)\n",
    "    plt.xticks(bar_positions, categories)\n",
    "\n",
    "    for i, value in enumerate(values):\n",
    "        plt.text(bar_positions[i], value + 0.1, str(value), ha=\"center\", va=\"bottom\")\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_horizontal_bar_chart(values, categories, xLabel, yLabel, title):\n",
    "    # Set the height of the bars\n",
    "    bar_height = 0.35\n",
    "\n",
    "    # Set the positions of the bars on the y-axis\n",
    "    bar_positions = np.arange(len(categories))\n",
    "\n",
    "    # Calculate the dynamic figure height based on the number of categories\n",
    "    figure_height = max(6, len(categories) * 0.8)\n",
    "\n",
    "    # Set the figure size dynamically\n",
    "    plt.figure(figsize=(10, figure_height))  # Adjust the width as needed\n",
    "\n",
    "    # Create the horizontal bar plot\n",
    "    plt.barh(bar_positions, values, height=bar_height)\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel(xLabel)\n",
    "    plt.ylabel(yLabel)\n",
    "    plt.title(title)\n",
    "    plt.yticks(bar_positions, categories)\n",
    "\n",
    "    for i, value in enumerate(values):\n",
    "        plt.text(value + 0.1, bar_positions[i], str(value), ha=\"left\", va=\"center\")\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to objects\n",
    "pet_documents = df.to_dict(orient=\"records\")\n",
    "records = []\n",
    "# TODO: Refactor\n",
    "for document in pet_documents:\n",
    "    converted_document = PetDocument(\n",
    "        name=document[\"document name\"],\n",
    "        tokens=document[\"tokens\"],\n",
    "        tokens_ids=document[\"tokens-IDs\"],\n",
    "        ner_tags=document[\"ner_tags\"],\n",
    "        sentence_ids=document[\"sentence-IDs\"],\n",
    "    )\n",
    "    converted_document.relations[\"source_head_sentence_id\"] = document[\n",
    "        \"relations.source-head-sentence-ID\"\n",
    "    ]\n",
    "    converted_document.relations[\"source_head_word_id\"] = document[\n",
    "        \"relations.source-head-word-ID\"\n",
    "    ]\n",
    "    converted_document.relations[\"relation_type\"] = document[\"relations.relation-type\"]\n",
    "    converted_document.relations[\"target_head_sentence_id\"] = document[\n",
    "        \"relations.target-head-sentence-ID\"\n",
    "    ]\n",
    "    converted_document.relations[\"target_head_word_id\"] = document[\n",
    "        \"relations.target-head-word-ID\"\n",
    "    ]\n",
    "    records.append(converted_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_tokens = 0\n",
    "for record in records:\n",
    "    total_tokens += len(record.tokens)\n",
    "\n",
    "def get_lengths(documents):\n",
    "    result = []\n",
    "    for document in documents:\n",
    "        result.append(len(document.tokens))\n",
    "    return result\n",
    "\n",
    "\n",
    "min_tokens = min(get_lengths(records))\n",
    "max_tokens = max(get_lengths(records))\n",
    "avg_tokens = total_tokens / len(records)\n",
    "\n",
    "generate_horizontal_bar_chart(\n",
    "    values=[min_tokens, max_tokens, avg_tokens], \n",
    "    categories=[\"Minimum\", \"Maximum\", \"Durchschnitt\"], \n",
    "    xLabel=\"Tokens\", \n",
    "    yLabel=\"\", \n",
    "    title=\"Anzahl an Tokens pro Dokument\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_tags = []\n",
    "for record in records:\n",
    "    ner_tags.extend(record.ner_tags)\n",
    "\n",
    "ner_tags_counts = Counter(ner_tags)\n",
    "\n",
    "plot_categories = []\n",
    "plot_values = []\n",
    "\n",
    "for string, count in ner_tags_counts.items():\n",
    "    plot_categories.append(string)\n",
    "    plot_values.append(count)\n",
    "\n",
    "generate_horizontal_bar_chart(\n",
    "    values=plot_values, \n",
    "    categories=plot_categories, \n",
    "    xLabel=\"ner-Tags\", \n",
    "    yLabel=\"Anzahl der Vorkommen im Datensatz\", \n",
    "    title=\"Verteilung von NER-Tags im PET-Datensatz\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_ner_tags = []\n",
    "plot_categories = []\n",
    "plot_values = []\n",
    "\n",
    "for item in ner_tags_counts.items():\n",
    "    if not item[0].startswith(\"I\"):\n",
    "        filtered_ner_tags.append(item)\n",
    "\n",
    "for tag, count in filtered_ner_tags:\n",
    "    plot_categories.append(tag.replace(\"B-\", \"\"))\n",
    "    plot_values.append(count)\n",
    "\n",
    "generate_horizontal_bar_chart(\n",
    "    values=plot_values, \n",
    "    categories=plot_categories, \n",
    "    xLabel=\"Anzahl der Vorkommen im Datensatz\", \n",
    "    yLabel=\"ner-Tags \", \n",
    "    title=\"Verteilung von NER-Tags im PET-Datensatz\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate HTML to verify\n",
    "with open('assets/template.html', 'r') as file:\n",
    "    html_template = file.read()\n",
    "\n",
    "data = \"\"\n",
    "\n",
    "for record in records:\n",
    "    html_content = html_template\n",
    "    html_content = html_content.replace(\"<!-- TITLE -->\", record.name)\n",
    "    for token, ner_tag in zip(record.tokens, record.ner_tags):\n",
    "        data += f\"<span class='{ner_tag}'>{token}</span>\"\n",
    "    html_content = html_content.replace(\"<!-- CONTENT  -->\", data)\n",
    "    with open(f\"out/{record.name}.html\", 'w') as file:\n",
    "        file.write(html_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate HTML to copy\n",
    "with open('assets/template.html', 'r') as file:\n",
    "    html_template = file.read()\n",
    "\n",
    "data = \"\"\n",
    "\n",
    "for record in records:\n",
    "    html_content = html_template\n",
    "    html_content = html_content.replace(\"<!-- TITLE -->\", record.name)\n",
    "    data += f\"<li>{record.tokens}</li>\"\n",
    "    # for token, ner_tag in zip(record.tokens, record.ner_tags):\n",
    "    #     # TODO\n",
    "    html_content = html_content.replace(\"<!-- CONTENT  -->\", data)\n",
    "    with open(f\"out/{record.name}.html\", 'w') as file:\n",
    "        file.write(html_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relations = []\n",
    "for record in records:\n",
    "    relations.extend(record.relations[\"relation_type\"])\n",
    "\n",
    "relations_counts = Counter(relations)\n",
    "\n",
    "plot_categories = []\n",
    "plot_values = []\n",
    "\n",
    "for string, count in relations_counts.items():\n",
    "    plot_categories.append(string)\n",
    "    plot_values.append(count)\n",
    "\n",
    "generate_simple_bar_chart(\n",
    "    values=plot_values, \n",
    "    categories=plot_categories, \n",
    "    xLabel=\"Relationen\", \n",
    "    yLabel=\"Anzahl der Vorkommen im Datensatz\", \n",
    "    title=\"Verteilung von Relationen im PET-Datensatz\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records[1].tokens[0:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_id = 13\n",
    "sentence_lengths = Counter(records[document_id].sentence_ids)\n",
    "last_pos = 0\n",
    "for sentence_id, length in sentence_lengths.items():\n",
    "    o = records[document_id].tokens[last_pos:last_pos+length]\n",
    "    print(o)\n",
    "    last_pos = length + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_template = \"\"\"\n",
    "Annotate the entire noun phrase mentioning the actor. \n",
    "In this context, an actor is any organizational element responsible for the action. \n",
    "If there are multiple actors, annotate each separately.\n",
    "\n",
    "This is an example:\n",
    "\n",
    "['The', 'MPON', 'sents', 'the', 'dismissal', 'to', 'the', 'MPOO', '.', 'The', 'MPOO', 'reviews', 'the', 'dismissal', '.']\n",
    "\n",
    "input: ['The', 'MPON', 'sents', 'the', 'dismissal', 'to', 'the', 'MPOO', '.']\n",
    "actors: [\"The MPON\", \"the MPOO\"]\n",
    "\n",
    "{input}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI()\n",
    "\n",
    "actor_input = records[11].tokens\n",
    "\n",
    "print(actor_input)\n",
    "\n",
    "prompt_template_actor = PromptTemplate(\n",
    "    input_variables=[\"input\"], template=actor_template\n",
    ")\n",
    "\n",
    "actor_chain = LLMChain(llm=llm, prompt=prompt_template_actor)\n",
    "\n",
    "response = actor_chain({\"input\": actor_input})\n",
    "\n",
    "print(response[\"text\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
